{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle,os\n",
    "import os.path as osp\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer=WordNetLemmatizer()\n",
    "from ohbm.api import Api\n",
    "import ohbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "access_token=\"Ask OHBM\"\n",
    "url='https://ww5.aievolution.com/hbm1701'\n",
    "api = Api(access_token,url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import hashlib\n",
    "# BUF_SIZE is totally arbitrary, change for your app!\n",
    "BUF_SIZE = 65536  # lets read stuff in 64kb chunks!\n",
    "\n",
    "def get_hash(filename, hashfunc=hashlib.sha1()):\n",
    "    with open(filename, 'rb') as f:\n",
    "        data = None\n",
    "        while True:\n",
    "            data = f.read(BUF_SIZE)\n",
    "            if not data:\n",
    "                break\n",
    "            hashfunc.update(data)\n",
    "\n",
    "    return hashfunc.hexdigest()\n",
    "\n",
    "#def _get_hash():\n",
    "#    if hashtype == 'sha1': \n",
    "#        sha1 = hashlib.sha1()\n",
    "#    else \n",
    "#        md5 = hashlib.md5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hash_of_file(filepath,hashfunc='sha1'):\n",
    "    import hashlib\n",
    "    BUFSIZE = 2**10\n",
    "    if hashfunc == 'sha1':\n",
    "        hashf = hashlib.sha1()\n",
    "    with open(filepath, 'rb') as f:\n",
    "        while True:\n",
    "            block = f.read(BUFSIZE) # Magic number: one-megabyte blocks.\n",
    "            if not block: break\n",
    "            hashf.update(block)\n",
    "        return hashf.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pckg_dir = osp.dirname(osp.dirname(ohbm.__file__))\n",
    "data_dir = osp.join(pckg_dir, 'abstracts_data')\n",
    "abstract_pkl = osp.join(data_dir, 'abstracts.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ABST_SHA1 = 'e8047573ad5f6b1f6aa079bf4de874f0cf53c969'\n",
    "\n",
    "try:\n",
    "    abstracts=pickle.load(open('abstracts.pkl','rb'))\n",
    "    # assert hashlib.sha1(open(abstract_pkl).read()) == ABST_SHA1\n",
    "    assert (ABST_SHA1 == hash_of_file(abstract_pkl))\n",
    "except:    \n",
    "    abstracts = api.Abstracts.getAbstracts()\n",
    "    pickle.dump(abstracts, open(osp.join(data_dir, 'abstracts.pkl'),'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "a = abstracts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "a.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Make a list of document, each is an abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fields=['title','results'] #,'materialsMethods' 'conclusion','title', ,'results'] # ,'purpose',]\n",
    "doc_list = []\n",
    "bad_docu = 0\n",
    "digit_regexp=re.compile('\\d')\n",
    "bad_code_regexp=re.compile('\\W')\n",
    "\n",
    "for a in abstracts: # abstracts[:300]:\n",
    "    #d = [a[f] for f in fields]\n",
    "    # remove document that contain None: they dont have strings for all fields\n",
    "    #if None in d:\n",
    "    #    continue   \n",
    "    try:\n",
    "        alltext = ' '.join([a[f] for f in fields])\n",
    "    except TypeError:\n",
    "        bad_docu += 1\n",
    "        continue\n",
    "    \n",
    "    alltext=re.sub(digit_regexp,' ',alltext)\n",
    "    alltext=re.sub(bad_code_regexp,' ',alltext)\n",
    "    \n",
    "    # d = [s for s in d if s] # remove None from list - may be remove abstract ?\n",
    "    # print(d); print(\"\\n\")\n",
    "    #doc_list.append(' '.join(alltext))\n",
    "    doc_list.append(alltext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "doc_list[1]\n",
    "print(len(doc_list))\n",
    "print(bad_docu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#doc_list[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import gensim\n",
    "from gensim import corpora, models\n",
    "\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "en_stop = get_stop_words('en')\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(en_stop)\n",
    "en_stop[:3]\n",
    "other_stops = ['p','left','right','brain','hemisphere',\n",
    "               'figure','significant','fig','activation', 'cluster', 'results',\n",
    "              'showed', 'based', 'may', 'found', 'subjects', 'age', 'aged', 'hiv', \n",
    "               'data', 'compared', 'matter', 'larger', 'smaller', 'sub', 'values',\n",
    "              'analysis','c','negative', 'positive','region','group','cortex','activity',\n",
    "              'areas','processing','using','cortical','et','al','sample','state','white',\n",
    "               'participants','available','mm','ms','image','task','can',\n",
    "              'across','studies','gm','wm','neuroimaging','gyrus','volume','method',\n",
    "               'condition','conditions','methods','method','model','fmri','mri','head',\n",
    "              'regions','will','regions','tr','approach','subject','peculiar','also',\n",
    "               'research','mean','non','signal','different','users','user','te','related',\n",
    "              'performed','perform', 'images', 'echo', 'one', 'two', 'three', 'four','five',\n",
    "              'six', 'seven', 'eight', 'nine', 'ten', 'eleven','twelve','use','used','new',\n",
    "              'dataset','datasets','associated','size','output','outputs','map','maps',\n",
    "                'study', 'studies', 'change', 'changes','en','effect','effects','differences',\n",
    "              'difference','voxels','voxel','time','times','score','scores','increased',\n",
    "              'increase','weight','weights','voxel',\n",
    "              'voxels','acquired','acquisition','acquisitions','whether','presented',\n",
    "              'measure', 'measured','enable','considering','described','describe','included',\n",
    "              'include','shows','showed','show']\n",
    "en_stop += other_stops\n",
    "en_stop += [letter for letter in 'qwertyuiopasdfghjklzxcvbnm1234567890']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_list:\n",
    "    \n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "    \n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    \n",
    "    # add tokens to list\n",
    "    #texts.append(stopped_tokens)\n",
    "    texts.append(stemmed_tokens)\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=25, \n",
    "                                           id2word = dictionary, passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(ldamodel.print_topics(num_topics=25, num_words=7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(ldamodel.id2word.keys()))\n",
    "ldamodel.id2word[ldamodel.id2word.keys()[500]]\n",
    "print(texts[0])\n",
    "ldamodel.minimum_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(corpus))\n",
    "print(corpus[100])\n",
    "ldamodel.minimum_probability = .01\n",
    "print(ldamodel[corpus[100]])\n",
    "ldamodel.minimum_probability = .001\n",
    "print(ldamodel[corpus[100]])\n",
    "\n",
    "ldamodel.num_topics/len(corpus)\n",
    "1./ldamodel.num_topics\n",
    "#ldamodel[texts[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tid in range(3): #ldamodel.num_topics):\n",
    "    topic = ldamodel.show_topic(tid,topn=50)\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
